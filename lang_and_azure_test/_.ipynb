{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.environ.get(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]= \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"RAG-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", api_key=os.environ.get(\"OPENAI_API_KEY\"),temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def vectorstore_from_pdf(pdf_file,chunk_size=1000,chunk_overlap=200,embedding_model = \"text-embedding-3-small\"):\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    pages = loader.load_and_split()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    splits = text_splitter.split_documents(pages)\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(model=embedding_model))\n",
    "    return vectorstore\n",
    "\n",
    "vectorstore = vectorstore_from_pdf(\"CCB NS 8415 Underentreprise, avklarende kontraktsm_te, referat.pades.pdf\",\n",
    "                                   chunk_size=250,\n",
    "                                   chunk_overlap=100)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG-Fusion:\n",
    "template = \"\"\" \n",
    "Du er en hjelpsom assistent som genererer flere søkeforespørsler basert på en enkelt inngangsforespørsel. \\n\n",
    "Generer flere søkeforespørsler relatert til: {question} \\n\n",
    "Output (3 søkeforespørsler):\n",
    "\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Den innkommende etterspørselen om å montere el-kanal gjennom himlingene er ikke gyldig i henhold til kontraktsdokumentene. Kontrakten nevner at \"Hulltaking i systemvegger for el ektrokanal langs fasade er inkludert\", noe som indikerer at arbeidet med el-kanalene allerede er dekket i prisen gitt i kontrakten. Derfor kan selskapet ikke kreve ekstra betaling for dette arbeidet.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "from operator import itemgetter\n",
    "template = \"\"\"\n",
    "Du er en assistent som skal sjekke om den \n",
    "innkommende etterspørselen fra et selskap med \n",
    "krav om betaling er gyldig i henhold til relevante kontraktsdokumenter, \n",
    "eller om arbeidet egentlig er inkludert i prising gitt i kontrakten. \n",
    "Begrunn hvorfor/hvorfor ikke den innkommende etterspørselen er \n",
    "gyldig i henhold til kontraktsdokumentene. \n",
    "Dersom kontraktsdokumentet nevner at \"prisen er inkludert\" betyr dette at selskapet ikke kan kreve mer penger på denne posten.\n",
    "Begrens svaret til 100 ord.: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Tittel\n",
    "El.kanal gjennom himling\n",
    "\n",
    "Beskrivelse av forholdet\n",
    "Det skal monteres el-kanal gjennom himlingene\n",
    "\n",
    "Utredning og forslag til løsning\n",
    "Kappet vegglister for el-kanalene i bygg F 5-6-7-8 etg\n",
    "\n",
    "Vederlagsjustering etter\n",
    "Regningsarbeid\n",
    "\n",
    "Kostnadsoversikt\n",
    "Postnr.\tBeskrivelse\tFirma\tEnhet\tMengde\tEnhetspris\tPåslag\tTotalbeløp\t\n",
    "1\tarbeid\tOBI\ttimer\t8\t660\t-\t5 280\t\n",
    "Totalsum alle kostnader\t5 280\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
